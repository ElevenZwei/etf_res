# TODO  

## 实盘数据录制
可以做的事情有很多  
把上一个录制机器搬到这边，然后做成 user 服务。  
注意数据的写入量问题，重复的数据还是不要写入比较好。  
昨天 2025-07-24 完全没有录制上，因为这个软件的设置需要日常更新。  
所以需要 Wind 弥补一下这一天的 8 9 12 月数据。  

新的录制数据库要怎么设计还是一个问题。  


## Call Put Ratio  
这个模型还有几个方向可以分析，  
对于郑心棠现存的交易方案，有这样几件事，  
一个是现有的几千组参数的调优处理，高维空间里面的聚点特征分析，  
一个是通过几千组参数的资产组合，获得低波动的高回报。  
这里面有一点就是 sum profit 单利和复利的相关程度分析。  

对于我原先的微观曲面模型，有这样几件事  
一个是分析时间切片归一化之后的分布稳定性  
一个是把之前的日内放大波动的曲面转换成归一化曲面  
然后使用已有的工具度量分布特征。  

这些事情其中很重要的一环是数据库的设计，  
良好设计的数据库可以轻松支持上万组参数的分析。  

## Daily Task

### 07-31 Task  
第一个问题是我们现在要做先后优劣程度的分析。  
分离做多和做空，简化到 open close -> profit 的三维曲面，  
对于 long short 还有不同的月份都要分别画一画。  
每月排名前五十名的策略统计一下其他月份的平均表现。  
我们想一想怎么从数据库里面取出对应的数据。  
先安装一个 neovim sql 插件。  

sql rolling args

### 08-05 Task  
现在有两个方向，一个是测试它混合的交易信号在期权上面的表现。  
另一个是分析它的收益曲面，这个需要提取目前参数之间的相关性，  
尝试聚合分离这些参数，从高位表达转向低位空间。  
在高维到低维转向的过程里面我们可以使用哪些工具呢？  

混合交易信号首先本身并不容易合成，我想一想它需要的记录方案。  
Rolling Args 表格本身的设计没有什么太大难度，  
Roll Result 要怎么合成出小数仓位又是一个复杂的运算工作。  
合成的最简单方法是直接带着 Frozen zone 去叠加 cpr.clip_trade_backtest 里面的 position 。  
这样就是数据读写量会大一点。

### 08-06 Task  
滚动组合回测的代码需要有几个地方。  
一个是 roll method 的代码  
一个是 roll args 的输入设计，另外还有运行时间范围的参数  
一个是 roll result 的结果表示和储存。  
一个是读取 clip_trade_profit 之后需要比较整合的过程，  
我们可以提供一个函数，输入所有需要提取的编号和时间段，  
返回这些编号这段时间的交易记录。  

运行 Roll 的时候，根据 trade_args_from_id / to_id 的数值，  
先读取这一段的 trade_args 表格内容。  
把这些表格内容输入 Roll Method 进行第一轮的筛选，  
这一轮筛选完成之后的列表，加上输入的时间窗口，  
获取到这段区域的 clip_trade_profit 或者 clip_trade_backtest 表格，  
把这段数据再输入 Roll Method 代码，  
让它按自己的评估逻辑做出排序结果，或者输出其他的分析报告。  
所以这个模块不仅可以滚动，也可以做其他的分析作用。  

这个排序会定期重置，所以它需要分段调用排序。  
我们可以在外层先调用分片函数，Roll Method 需要提供一个分片逻辑。  
分片逻辑先把时间分成片，然后在每一个时间片上调用排序函数。  
这是静态分片的方法。  

也许需要有根据现有的排序结果里面的收益变化，改变分片长度的动态触发。  
为了满足动态的分片和动态的排序，不是每个分片单独孤立的静态算法排序。  
我们在一轮的排序触发的时候，应当告知上一轮的排序结果在验证集上的表现  
然后它给出新的排序，和这些排序维持的时间，时间到期的话再触发一轮排序。  
或者说它给出的排序一旦验证集上触发了什么条件，那么立刻进行新的一轮排序，不等到期。  
所以这就是另外一种不同的 Roll 运行形式了。  

这两种形式再 cpr.roll_method 表格里面使用 is_static 数据列区分。  
is_static = true 的运行需要 args 里面提供 validate_days, train_days_factor。  
它会使用你提供的时间长度进行分片，  
train_days_factor 表示训练集相对于验证集的时间长度的倍数。  
当 validate_days 是 7 的时候，自动把开始的日期对其每周。  
当 validate_days 是 30 的时候，自动把分片对齐到每月。  
它分片完成之后，在每一个分片上调用 roll method sort 函数得到这一段验证集的预测排序。  
排序的输出需要权重，所以它应当输出一个 DataFrame, 这个 DataFrame 里面需要三列。  
再加上一个分数评估用于日后的比对。  
id(int), rank(int), weight(int), score(float)  

我们先把上面说到的部分完成。

### 08-07 ~ 08-11 Task  
现在主要的问题是需要为了实盘做准备，实盘十组参数会有十个开平仓的时间。  
于是我需要一个叠加参数的，从实盘数据库抽出数据，读取叠加的参数，然后开出单子。  
目前设计的工具流需要这样几个方向，先从总体的流程上思考。  

实盘信号程序和实盘交易程序分开，实盘信号程序负责得出和储存交易信号，转换成现在的仓位数值，实盘交易程序负责把这个仓位数值转换成仓位。  
实盘信号程序需要读取十组参数，这十组参数需要上个星期的回测得到。  
回测的数据需要从录制数据库导出分钟级别的 Call Put Open Interest 还有 ETF 价格。  
目前这段导出的代码是 `./datavis/oi_scripts/db_oi_csv.py` 。  
然后还需要导入 cpr 数据库，这一步需要自动化起来。  
然后从 cpr 数据库运行回测也需要自动化。  
之后是导出 cpr 回测得出的数组，这是一个参数编号数组。  
这个数组需要转换成 cpr ratio diff 的触发数组。  
关于触发数组如何转换成仓位的问题，我需要写一个 python 代码，  
这个 python 代码要在回测数据上做验证，保证结果和我们的多组平均是一致的。  
然后这个 python 代码要保存到实盘数据库，交易程序要读取这个实盘数据库。  
交易程序从实盘数据库到仓位的转换变化过程可以参考 Nautilus 回测代码。  

另外现在得到十组参数的代码，因为要写入 roll_rank 表格， 所以它需要已经有完整的验证集数据。  
但是这样就不能预测实盘参数了，所以我需要设置一个模式不写入 roll_rank 的单纯预测模式。  
然后是得到十组参数的代码现在有很强烈的相似性，分离这些不同参数的收益特征需要 PCA 统计。  
经过大量的去重之后得到有差异性的参数组合才能反映问题。  
这个过程如何进展？这一点还需要很多的统计。

上面说的是流程上的东西，然后我们需要架构上的设计。  
实盘机器用哪个？录制市场数据的数据库用哪个？信号计算放在哪个地方执行？交易信号的数据库用哪个？  
这里问题的关键是我们使用 CTP Option 互联网实盘还是使用 Huaxin 实盘。


这个真的是任重而道远。

我需要整理一下各种鬼地方的 Wind 下载代码，  
现在有好几个地方，我应该汇总一下功能，  
或者应该先研究工具流的问题？比如说 RIME 输入法的配置同步。  
仔细判断一下现在数据流的输入问题，我们使用的数据如果和 Wind 下载的数据做一个对比？  

### 08-12 Task  
昨天搞定了 Position Merge 的代码，今天进一步思考实盘运行起来的必要条件。  
实盘运行的方式现在暂定成 Export 所有需要的参数，然后新建一个脚本运行在 Record Server 上面。  

Export 是一种整理信息的方式，即使不做 Export 在程序运行的时候差不多也要经历一个 Export 过程。  
Export 需要的参数这个会真的很多很长。  
比如说有十组参数，每一组都需要每一分钟的分布，  
或者说每一分钟的触发阈值，所以说这需要一个 Dict 结构。  
像是 cpr.clip_trade_backtest 表格里面的一样。  
我们用这样的结构表示：  

```json
{
    "dataset_id": 1,  
    "input_dt_from": "2025-01-13 00:00:00",
    "input_dt_to": "2025-01-14 23:59:59",
    "interval": 60,  
    "roll_args_id": 1,  
    "roll_dt_from": "2025-01-13 00:00:00",
    "roll_dt_to": "2025-01-19 00:00:00",
    "roll_method_id": 1,
    "roll_method_name": "best_return",  
    "roll_method_variation": "logret_t1w_v1w",  
    "roll_method_json": "<json dump>",  
    "roll_trade_args_from_id": 1,  
    "roll_trade_args_to_id": 8082,
    "spotcode": "159915",
    "top": 10,  
    "trade_args": { "1000": 0.1, "1001": 0.1, ... }  
    "trade_args_details": [  
        {  
            "trade_args_id": 1000,  
            "trade_args_json": "<json dump>",  
            "trade_args_thresholds": {  
                "long_open_threshold": 0.3,  
                ...  
            },  
            "trigger": [  
                {  
                    "time": "09:35:00",  
                    "long_open": -0.002,  
                    "long_close": 0,  
                    "short_open": 0.002,  
                    "short_close": 0,  
                },  
                { ... },
            ],  
        },  
        { ... },
    ],
}
```

交易程序读取里面的 trade_trigger 数组，然后把它转换成 DataFrame，存放在指定的 spotcode 名下。  
当触发 spotcode 的新行情的时候，检查这些 DataFrame，如果这个 DataFrame 里面有分钟数字对齐的条目，那么接着检查这个 DataFrame 的 open close 数字，和当前行情的 `(c-p)/(c+p)` 的日内变化量做比较。如果触发的话，表示这个 trade_args_id 需要变换仓位。  
每个 trade_args_id 的仓位从 0 到 1 到 -1 ，在输出的时候再按照权重加权计算总和。  

触发 spotcode 新行情的方案，按照 interval 字段的间隔触发，不过为了和回测对齐，在触发的时候应当尽可能贴近分钟开始的时候，考虑到延迟等等情况，我们可以在 5 秒的时候检查数据库里面是不是有新的行情。或者说经常查看行情数据库，在看到新的行情和上一次触发时间之间的时间差满足条件的时候就触发。  

每次检查触发的最后，给当前的状态 DataFrame 新增一行每个 trade_args_id 的检查结果，并且把这个 DataFrame 保存到 csv 文件里面。csv 文件的命名不能重复，例如用启动时间加上 spotcode 命名。  

```text
CSV 数据格式表达。
filename: cpr_pos_spot_159915_dt_20250808_092000.csv
csv header: dt, spotcode, position, ta_1234, ta_1235, ...
```
（最后我没有使用这么复杂的 DataFrame 格式，我使用的方法是分离几个 Trade args 分开判断之后再加权组合。）

这个记录储存在数据库里面的话就需要使用一个转轴的写法了：  

``` text
SQL 数据格式表达。
dt, spotcode, trade_args_id, position, weight  
其中 trade_args_id is null 并且 weight = 1 的数据行表示仓位总和。
```

检查触发之后还需要其他的形式通知交易软件，要是交易软件每次都去读取文件 File Descriptor 的变化那就有一点太复杂了。那不如我们单独搞一个数据库，例如 Redis 共享 Dict 的方式，或者 SQL 查询最新一行的方式。  

于是今天我们把配置导出和读取市场数据的这样两步给解决了。  

Export 脚本需要读取 Roll Result 表格的数据，然后查询每个 trade_args_id 的详细配置，再从 cpr.clip 里面读取对应的分布情况。好像相当复杂。  
这个脚本有很大一部分功能和 `cpr_diff_sig.py` 是重叠的。  
Export 脚本为了得到触发表格，它的日期范围只能限制在同一个星期里面，用日期加上 trade_args 里面的 date_interval 数字得到分布采样的日期范围。然后再加每一分钟得到 dt_range_id。然后再查询 `cpr.clip` 表格得到采样数据。用这个采样数据和 `cpr.clip_trade_args` 里面的触发条件做比较，得到具体的触发阈值。  

现在 `clip_trade_args` 里面的 args json 是这个形式。
```json
{
  "method": "min_max",
  "variation": "default",
  "arg_variation": "c45_lo30_lc10_so20_sc0",
  "date_interval": 90,
  "zero_threshold": 0.45,
  "long_open_threshold": -0.3,
  "long_close_threshold": -0.1,
  "short_open_threshold": 0.2,
  "short_close_threshold": 0
}
```

### 08-13 Task  

Export 得到的阈值首先需要和 clip_trade_backtest 里面的数列进行对比，确定得到的触发边界是不是一致的。嗯，验证过了，是相同的。  

Export 的脚本终于搞定了之后，下一步是需要解决这个脚本的运行机制，它需要可以从数据库里面读取指定时间的 Open Interest 数据，自行计算仓位变化的信号，然后我需要把这个仓位变化的信号和我的 roll_merged 表格里面的数据对比。

`src/json_run.py` 做出来的结果看起来和 roll_merged 表格里面是一样的，这样很安心。下一个需要验证的是 roll_merged 仓位的收益是否和 Logret 报告的复利收益一样。  

### 09-12 Task  
实盘有几个 Critical 级别的问题，一个是关于 Market near open close 的判断问题。后来发现是 Huaxin Ctp Options 接口特有的得不到 ETF 合约详细信息导致的检测漏洞，已经修复。  
一个是总体仓位保证金的监控和报告机制。  
一个是各个策略的营收表现的报告机制。  
这些都做成一个个 Spirit 模组。  
另外一个老大问题是录制机器的移动，这个优先级目前往上调。  

### 09-15  
关于安全警告性质的监控系统。  
有几个方面可以做，一个是说关于之后一个交易日是否有足够参数的检查。一个是关于持续运行的时间和数据库里面是否有合理数据的检查。  
然后比如说周五下午可以自动执行一次 weekly update ，这个脚本自己也需要内部检测，保证每天都有充分数量的信号，不是哪一天特别残缺，所以者需要先经过一个数据完整性的检测。  

### 09-18  
今天折腾了一天才知道在本地运行华鑫内网交易程序的办法。  
基本上是使用 `ssh -D <port>` 建立 socks5 隧道，然后使用 proxychains 代理所有的程序出站连接，但是 127.0.0.1 的除外，从而保证对本地信号数据库的连接。  
仅仅转发一两个端口的数据是没有作用的，它会使用的端口不限于那一两个，仅仅一两个端口就会报告连接断开，发送失败等等。当我把 hx 机器上的程序复制下来发现没有办法运行的时候，我就意识到了这个问题。  
同时 hx_yl 这个机器上不能运行产品号以外的任何账号，所以我只能从 hx_yl 机器上面监听行情，但是不可以发出交易。  

目前的情况是我建立了整个 proxychains 配置文件之后使用脚本启动交易程序，这样就可以正确运行了。  
我最好需要判断一下这个程序建立了哪些连接。这个可以用 proxychains 来帮助我记录吗？proxychains 有没有这样的日志功能，像是 proxifier 一样？  


另外一条路线是在服务器里面配置完整的运行系统。
在服务器上面配置完整的交易系统遇到了这样几个方面的阻碍：  

1. 编译的时候 CPM 需要使用 env proxy 才能连接互联网。使用 `ssh -R 7888:localhost:7888` + export proxy 的方法。  
2. pqxx 需要依赖 libpq 才能使用，在没有系统包安装权限的场景里面需要手动编译 libpq 。  
3. libpq 并没有单独精简的 git 仓库，它是和 postgresql 混在一起编译的。它会依赖一部分 postgresql 的编译内容。而 postgresql 编译的依赖项目比较多。有 icu, lex, bison 这些开发工具。这些东西如果不能装系统包的话都需要一个个手动编译。  
4. 这些手动编译的工作量一点也不小。而且编译之后都放在 `~/.local/` 文件夹里面，然后需要配置系统环境变量让其他的工具可以搜索到这个位置。  


我不知道上面这些配置过程可否使用 AI Agent 工具自动搜索资料下载编译试错解决问题等等。  

### 09-19  
为了将来自动化的表达，我还需要解决这样一些问题：  

1. viu ssh 图片显示，搭配上 df plot 和 ai copilot 一起使用，不再重复本地化的 Excel 绘图工作。  
2. 自动化回测结果的报表展示，使用上生成回测结果分析的软件包 quantstats，最好能每日更新。  
3. 每天要启动的软件的日常服务化。  
4. Hummingbird 启动之后写一个 lockfile，然后 grafana 里面也要监控好打点输出的情况防止多开。  
5. Hummingbird 每日启动之后的打点，交易记录，还有当前仓位的自动上传到数据库。  

### 09-25  
今天在执行的时候没有开出仓位，反思一下是因为没有做好版本控制和管理，这个比较复杂，因为它同时涉及数据库层面的生产和测试的分离，还有软件环境执行层面的分离。  
如果说需要一个生产数据库的话，那么应该再建立一个 database ，然后只执行 weekly_update.py 计算生产数据。此外所有的更改都先在测试数据库和测试账号上确认可行了之后，才能推送到生产数据库上。  
在此之后的发生原因才是时间区间的边界没有做好分析和判断。关于下一个日期是否初始条件合理的测试可以直接写在 grafana 里面。  

版本的分离: dev (sim) -> prod (1810) 两个账号。  
数据库的分离: opt -> opt_prod 。对应的 python 脚本和 Hummingbird 连接信息需要更新。  
软件版本的分离: hb -> hx_s1_prod，需要更新 hb_daemon 脚本。  

今天试着把这些工作完成。  
根本完不成，今天修了很多细节的触发问题，还有调试了新的数据库库工具链。  

### 10-11  
今天需要设计一下股票信号和期权信号的组合方法。有一些什么样的模型和工具可以让我使用呢？  
第一个工具是输入组合之后的信号可以很快得到盈亏统计的脚本，现在的工具用 Nautilus 太慢了。  
我们如何表示盈亏，一个是可以使用每日变化来表达。  
如果我们使用遮蔽模型，或者 max min 模型，或者说他们两个的差异来判断。  
股票信号的基本特征是冲击响应模型，在冲击不持续的时候就会慢慢减仓。  
我们需要评估它们信号之间的相似程度和一致性。  

此外，关于方法论和更多新的模型，要按照这个笔记开头的说明再调整。  

于是我们分解成几个容易完成的小任务。
1. 输入一组 `[-1, 1]` 之间的信号和一个标的，输出每天的盈亏。  
2. 尝试各种组合信号的方法，然后跑一跑这个盈亏。  
3. 先手动尝试一些简单的组合方法，找一些方向之后再设计更加系统性的中试工程。  

#### 10-15 update  
stock 和 cpr 的收益线性相关性非常大，在社科调查里面这样的数据已经可以算作显著相关。  
累计营收的线性相关是 0.96 ，每日营收的线性相关是 0.66 。这表示这两个信号之间一定存在某种我们不知道的信号通路。  
尽管是显著相关，但是这两个信号在微观特征上有一些区别。  
cpr 信号的特点是如果上午有强烈趋势就会一直持有到收盘，中间平仓的机会不多。如果没有趋势就会一直空仓。jk
stock 信号的特点是在趋势日子里面，它可以做到见好就收，但是在震荡的日子里面，它容易反复上套。

最近得到的结论是不同的分歧程度下，这个 stock 和 cpr 的准确性有偏向。  
但是这个偏向的度量方法和均匀程度，对于我找出来的参数会不会过度拟合这个问题。  
这个样本结论的假设检验问题还需要验证。  
就像最近我看选出来的 trade_args 相似程度很高，实际上我应该找一些相似程度不是那么高的。  
收益的假设检验模型是一个问题。  

### 10-20  
hb spirit catcher 在所有的 spirit 都静态声明不改变持仓的时候应该跳过下单。  
这个包括录制过程里面的东西我都处理好了。  

### 10-21  
今天有几个任务，  
第一个是长期运行的程序的服务化，这个服务化需要用 link 的形式指定它运行哪个版本的程序。  
第二个任务是关于今天上午的吃单无效的滑点分析，今天上午到底产生了多少的熔断以及跳空。  
在熔断的时候市场收到的数据是什么样的？在熔断的时候是不是应该直接放弃下单？  
熔断常常伴随着转折，是否拖延下单，比仓促地换行权价下单更加妥善？
第三个需要解决的问题是今天的保证金为什么算出来变成了 18 手。保证金不够的时候应该怎么下单？这个是之前没有完成的保证金调整的部分逻辑。  
第四个是研究下单过程里面的平均滑点冲击法，根据吃单成本控制当前的下单数量，不再使用固定的数量下单。  

服务化的任务里面最重要的是监控体系，监控体系的建立还需要很多脑子。比如说程序的中断，程序的报错处理。  
很多事情需要更加系统化的运行方法。  
这在这个之前，更加重要的是增加对于当前账户资产和持仓的记录。
一种更加体系化的方法是？日志服务器。  

### 10-22  
先解决滑点和市场深度的问题。研究这两天的平均吃单深度。我们把 delta 最靠近 0.5 的期权选出来。然后思考下单深度产生的成本，比如说我们要把成本控制在 0.03% 的话最多可以吃多少深。  
按照 30000 一手来算，0.03% 的成本要求期权组合的两手各自距离中间价不超过 3 * 3 / 2 = 4.5 元。  
我们回测里面估计的无摩擦日均收益是 0.2% （年化 50% ETF ），经过期权合成期货放大之后是日均 1%，假设我们可以接受 20% 作为交易成本的话，这个时候一天交易的成本控制在 0.2% ，那么一只期权的成本就是 0.05% （开平仓一共四手），那么就是成交距离中间价不超过 15 元，去掉手续费是 13 元。因为吃单的不确定性，我们按照 10 元的吃单范围来估计每个 Tick 的容量。  
这么看的话，仅仅记录 3 档盘口是不够的，Record 系统需要记录五档盘口。我们今天的估算按照三档盘口先进行。  
平值附近的三档盘口看起来可以提供大概 200 手的容量。五档盘口保守估计有 500 手，那么滑点 10 的话应该有 1000 手。  
1000 手表示一个 Tick 可以吃掉 500 万的资金，但是我们真正的问题是这个冲击下去之后的补充速度如何统计。  
统计的对象是 100 手，200 手，300 手吃单成本这个数字。  
（不不不，后来发现五档盘口在中位数上面只有 40~70 手的深度，那什么几千手都是极端数字。）  

在这方面加强监控，我们需要监控的是超高速 0.5s 五档口的记录，这方面的功能还需要一定的开发。  
另一个问题在于我们只能知道自己下单成交的记录，我的录制工具没有市场实时成交信息，这个也许可以从 Wind 找到，统计大单成交之后的挂单变化和补充速度。这个补充速度按照什么记录？因为大单成交都伴随着底层标的的价格变化，所以补充速度不是原样恢复，一定是双边的移动。我们记录这样一个双边吃单成本的变化曲线，一个开合线的形式。  
这个部分的实时计算（平均吃单价格）放在 Record.cc 里面完成，我们增加一些关于吃单情况的表格。  


现在还有对锁仓位和全平仓位的问题。  
我需要加一个运行选项，让它在收盘的时候处理掉对锁仓位。  
然后收盘的时候 0.0001 的权利仓可以不平仓，因为平不出去的。  
最近经常发生 retry abort 的情况，我想一个是因为这个收盘平不出去，另一个是因为我的 retry 模式不是按照市场价格 retry 的是吧。  

#### update 10-24  
0.0001 拒绝下卖出单的功能已经加上了。  

### 10-23  
今天的问题是又忘记换月了，而且今天增加了新的监控压力，我需要处理几种监控，  
一个是信号在 ETF 上面的理想收益监控，一个是账户的净值监控，一个是实际成交价格和信号触发前后的滑点差异。  
现在主要执行的任务按什么顺序完成呢？  
先解决每个月都要手动设置换月的这种负担，还有今日份的 Wind 数据补全和回测。  
然后是接入新的数据源，用新的录制系统驱动信号生成。新的录制系统比之前的更加稳定。  
再做账户净值的上报。  

之后因为发现了 previous oi close 和 oi open 在 md.contract_price_minute 表格上面的差异，
所以给这个数据表加上了 oi_open 和 vol_open 数据列，我觉得这个对于会跳空的期权还是比较重要。  

但是我意识到一个问题，在新版的带有去重功能的录制机器里，oi_open 这个数据列不是这一秒钟开头就会写入的，也就是说这个数据列在这一分钟的不同时间点里面获取的数值会有变化，这就要命了。对于从 Tick 获取数据经过 pivot + sum + resample 压缩的数据还好，因为 spot 总会在分钟开头得到行情，然后 pivot + ffill 总是可以在 oi_open 填充上一分钟的最后一个行情，但是对于实时压缩的数据，这个就不一样了。  
TODO: 所以对于从实时压缩的数据里面获取 oi_open 的话，要结合它的 inserted_at 时间戳，如果这个时间戳距离分钟的开头很近的话，那么这个是可信的，否则我们需要填充上一分钟的 oi_close 更加可信。

### 10-24  
今天发现只下了 12 手单，我感觉是在 delta pick 的过程里面选择到了一个不是那么平值的期权，导致保证金占用在计算里面上升了。所以 delta pick 的过程还是应该从平值向外推理更好吗？当我 pick delta 输入的是 0.4 的时候，我就只该在虚值+平值的范围里面寻找。输入 0.5 的时候就只考虑平值的两条期权。  

今天还有一个问题是我的几个交易策略需要支持同时选择几个不同档位的期权，这对于增加策略容量至关重要。  
今天解决的是写好了新的月份解析，还有 A-Type Option 的分类和数据录制问题。  
看起来下一个任务一个是 delta pick 的改进，另一个是实时真实仓位（分开权利仓和义务仓）的上传，还有实时账户净值的计算工作。还有理论收益的计算程序。还有这些系统的后台服务化。  
后台服务化之前又是日志服务器的建立，整个工作真的是任重而道远。真的一点也不能有别的心思了。  
还有关于交易失误的分析，交易不正常滑点的分析流程，现在还需要一个比较好的方法。这方面最好的方案是自动化的 Nautilus 每日交易统计，但是最近实盘的交易细节调整也需要更新到回测代码上面。  

实时 ETF 理论收益可以用 cpr.roll_export_run 的实时结果和 md.contract_price_minute 这两个合成出来，合成的脚本也有，就是那个 ./cpr/src/sig_worth.py 我们把这个合成出来的结果放在另一个表格里面。

### 10-26  
关于对锁仓位，现在看起来比较合理的做法是在下单的模块里面增加一些自定义的组件。  
我们需要统计每日的成交数量，和我现在需要多少个对锁仓位的数量。一旦动用了这个功能，感觉只能开一半一下的保证金了。计算是 10 手合成期货会占用 10 个卖权保证金，交易数量至少是 40 手，如果一天以内开平多次可能会上升到 80 手，80 手需要至少 27 个对锁仓位，也就是 27 个卖权的保证金。

### 10-27  
今天最直接的任务是导出 Stock Signal 放在 sig_worth 脚本里面跑一跑，把最近的 pcr 信号也跑一跑，先得到实盘以来这几天的真实 ETF 收益，然后看一看真实的盈亏是不是和 ETF 收益稳定在一个倍率上面。  

150 万资金开出 850 万的创业板空头。  
现在的 Nautilus 回测简直是一团乱麻，我还需要建立一个更加干净整洁的回测空间。  
之前建立 dsp 的代码有问题，会导致行权日期没有市场数据。  
这些问题这两天里面全都修好了。

### 10-29  
今天最重要的事情是需要解决 1000 手以上的收盘存量问题。  
在对比信号的的时候发现一个问题，Stock Signal 的信号如果放大，反而会拉低收益。CPR 的信号放大的收益基本不变，但是这两个信号的平均值一旦放大，收益会如同火箭冲天一样的上涨。  

今天发生了非常意外的事情，我中间所有的交易平台的优势要被人夺走了，尽管他们没有提供任何文件的收益对比，但是他们提供了一个可以给交易员使用的操作平台，这样的优势是我以前没有设想过的，我必须要有自己的保留地，这个保留地应该是什么都行？  
我可以做到的最好的目标是我仍然保留自己的交易能力，但是让它们执行关于监控和分析的过程。  
或者只让它他们的系统提供手动交易的而能力，我这里仍然处理稳定的自动交易。  
但是我为了完成这个目标我应该做到一些什么？这他妈的完全变成了竞争压力。  
我现在对于别人的信任程度大幅降低。我甚至觉得自己知道的信息完全不能透露给别人。  

### 11-03  
现在最重要的任务是关于开仓平仓以及产品化需要的监控模型。  
其次的任务是实时的行情显示以及收益统计，还有实时的 Trades 累加显示。  
要统计一下交易量，统计的方法是 sum(abs(signal_diff))  

### 11-05  
今天要手动填写一下早上遗失的几笔交易 1810 开仓的交易遗失了，然后 1915 有两笔一两手的交易遗失了。  
各种各样的信号的组合形式如果可以有实时的盈亏表达就好了。这个就是之前想要做但是没有做到事情。  
最近的身体状况各种都是炎症反应，真的是我的免疫力太糟糕的， 一不小心就会被奇怪的细菌感染。

### 11-08  
我需要更新一下回测里面使用的 399006 Signal ，统计完整的从年初到今天，看看 Stock 这一周的真实盈亏。  
无摩擦情况下 Stock 在 11-03 ~ 11-07 这一周的盈亏是 -0.1% ，CPR 是 -2.7% 。现在实盘来看 Stock -6%, CPR -16% ，这其中的每日损失是怎么来的？这个问题要刨根问底。  
Stock 在 11-07 可能有执行上的问题，这一天的理想收益是 -0.3% 但是真实收益是 -3% 。上午整个卡在死循环里面了。
分析这种随机损失的方法，还要之后用于在回测模型里面真实把握随机损失的随机模型，这两个方面都需要强化努力。  
明明问题特别特别多，最近的工作却很懈怠，不知道是什么疾病的原因，还有 polars 需要掌握，还有 C++ 的科学计算诸多。  

Hummingbird 还需要加上 Chain Trigger Count 和 Heartbeat 两个数据的打点上传，这个有助于我们确定重启时间。  
因为现在设置中途一旦断开连接，宁可重启也不要等待重连还原状态。  
这个引发的问题是随机的重启会非常影响还没有成功上传的缓存数据，我最好先设置一下 throw 之前的 std::wait ，这一步完成了。  
我可以把无聊的 journal 全部关掉，但是现在的问题是，重启程序的时候没有订单的全部刷新重置，也没有把订单跟踪还原的机制。  
要是重启完成之前订单都被消化掉了那还好，要是没有消化掉的话，那么剩下的订单会变成没有办法跟踪的野订单。  
解决这个问题的方法是下单的整个方面重新设计。  

在下单的时候写入一轮数据库，在下单收到回报的时候再更新，这样一点点更新。如果数据中断的话，就从中断的 order 状态还原。  

今天导入 Stock History Signal 的时候出现了一些操作失误，让这些数据覆盖了 09-30 到 10-14 其中的实盘数据。  
我现在需要比对没有覆盖的部份的一致程度，是只在收盘的时候有差异还是中间其他地方也会产生差异？  
妈妈的，我发现整个都有差异，根本不是收盘的时候有差异，他给我的历史信号是没有均线处理的跳跃信号。但是它实盘里面给我的却是有均线处理的均匀信号。  
这样的话 cpr.stock_signal 表格在 10-14 以及之前的数据都会和实盘读到的数字不符。

### 11-11  
我发现实盘的收益的总是可以分割成 intrinsic value 和 time value 两个部分，intrinsic value 的变化应当和 etf profit 维持在稳定的倍率。  
这个倍率的波动仅仅和开仓数量，还有 twap 产生的价格浮动有关。  
time value 的波动包括 put call parity 和 slippage 两个部分。  
intrinsic value / time value 得失可以从 market data 和 trades 记录里面得到，但是这个不是实时刷新的，只是交易产生的变化值。  
想要实时刷新的 intrinsic value 得失的话，需要比对交易时刻的 value 和 trade sum value 差值，这个也可以计算出来。  

实时的 etf 得失用 SQL 计算有点复杂，用 sig_worth.py 背后一直运行会比较好。  
用 SQL 计算的话，需要把 signal 转换成 etf 开仓价格，然后和当前价格对比，计算 net_1_intraday 效果可以。

### 11-17
今天早上是 BuyOne 忘记设置 setConfig() 函数，导致整个程序都没有启动起来。这种情况真的是太扯淡了。我需要设置一个预警方法，比如说设置 try 包裹 spirit 控制问题范围。比如说出错之后及时报错，而不是我手工发现没有交易之后再去查询。  
另一个问题就是新版程序的推送范畴，我还是应该先在 Paper Trading 上面试验。就算试验成功了也会因为 Paper Trading 和现实的 broker 不一样导致难以理解的问题，所以真的还是需要灰度更新。  
今天的 CtpOptions 为什么总是说 buyone not configured 我还没明白，之后明白了，是 clear 函数不能去清除之前设置的配置引用，因为 clear 函数调用起来太常见了。  


然后的下一个问题是关于仓位恢复的问题，今天出现了一个我以前完全没有想到的情况，因为程序总是想要恢复上一次还没有实现完毕的仓位，如果那个仓位是开不出来的话（例如资金不足），它就会卡在这个死循环里面。具体来说循环是这样的：  
读取上一次保存的仓位 >> 仓位不是空的，所以不会重新挑选期权计算最大值 >> 读取上一次保存的最大值，用最大值计算新的仓位 >> 计算的结果就是保存的仓位 >> 尝试恢复保存的仓位 >> 恢复不了，程序重启。  
怎么还能有这种问题，这种问题要怎么修复啊。  
这个只有一个办法可以解决，那就是在报错的时候，下单出错或者撤销出错的时候都要把出错的原因匹配到一个通用类型上面。然后把这个通用类型的报错添加消化处理的逻辑。

今天有一些资金不足开不出来的原因是因为平仓没有在开仓之前完成， 因为有冷却时间的原因，平仓和开仓在不同的 tick 里面轮流下单，导致资金不够用了，这个问题我现在已经修复好了。但是这个需要先把 BuyOne 的 configure 问题解决才能推送下去。这个搞定了。

今天还总是遇到需要清空 persistence 表格的情况，这个原因是因为 signal 不变的时候不会去刷新期权最大值，所以之前计算的最大值会一直产生影响。  
然后周五试验的时候还把 init frame count 调整到了 1 ，这个结果就是今天根本算不出来保证金，结果导致最大值都不对。  
这个问题的真正解决方案只有生产环境的配置选项和测试环境彻底完全地分开。  
仔细想一想，循环要怎么写才能不是在问题里面来回循环。  

1. 错误代码的反馈，有一些是不值得重试的场景，我们应该直接放弃重试。  
2. 放弃重试之后的 Hold / Commited 数据要怎么和外界持仓数据校准呢？我们需要一个完善的方法。  
3. 在下单之前需要一个总体风控校验步骤。有两个部分，一个是确认资金可以开出仓位，另一个是确认这些仓位可以被执行，例如 0.0001 的卖出就应该直接被驳回。我们应该有一个驳回 desired position 的机制。  

第一步还是要使用测试账号得到完全稳定的结果。  
今天 3129 有一些交易没有上报到数据库里面，为了解决这个问题，比较好的方案是不要让它经常 crash，只在最优必要的时候才重启。也就是说还是应该用 try 包裹 spirit 如果没有得到有效结果的话就保持之前的仓位。  

最近的所有策略的回测结果都不是特别理想。但是这方面的历史数据更新还是一个问题。  
所以任务全都堆起来了。  

1. import stock signal and run backtest.  --ok  
2. insert missing trade record on combined signal 11-17. --ok   
3. wrap spirit with try-catch block and report caught errors. --ok  
4. add start / trigger counter to Hummingbird.  --ok
5. fix net worth calculation glitches.  --undetected  

### 11-18  
第一个任务的后续发现。按这两个月的表现，CPR 双周轮换的表现好像比单周轮换更加理想。  
第二个任务的后续是写了一套从 log 提取 trade_id 然后对比数据库里面的现有数据，再写入丢失数据的代码。这些代码在 hb_investigate 文件夹里面。  

### 11-20  
这两天意识到一个问题就是计算 greeks 不能用现货的价格，需要使用期货的价格。期货的价格需要使用 Call Put Strike 三者隐含计算出来。我先需要用历史数据计算一份，用这一份和当前市场上面的现货价格做对比，确认一下偏差值。如果这个算法在历史数据上面没有什么问题的话，然后再推送到实时行情里面计算。  
这个任务也需要快一点完成。

新的 todo list  
1. 11-21 有好几次程序重启的问题需要调查。  --ok 都是熔断导致的，经过新的熔断阻断下单的机制之后这种情况没有再发生过了。  
2. 新的回测和每日收益对比的方式，或者说从每日采样的账户数据里面进行分析的脚本。  
3. Hummingbird 上传数据库的时间戳太喜欢使用 to_timestamp(std::time(nullptr)) 这样的写法，这种精确到秒的东西迟早要出事，所以数据库里面的 trade id 的顺序才会混乱。我之后需要仔细拦截每一个 order change 和 trade 信息的时候肯定需要更加精准的时间戳。上传 order change 可以让我了解每一个报单的细节表现。  
4. 关于执行层面的问题，我想我需要一个更加好的交付系统，比如说运行的时候使用后台服务，编译的时候交给自动推理的机器运行。然后这里最需要处理的事情是在出错的时候的回滚动作。比如说配置文件的版本管理，比如说陷入错误循环里面的时候自动清除过去的状态，这种自动清除，我们首先需要一个关于非正常运行过程的标记方法，比如说重试的数量，比如说重启的数量。  


### 11-26  
最近都在处理奇怪的故障问题。  

最近新的 todo 有这样几项：  
1. 提前一天换月的功能实现。  --ok
2. 11-25 / 11-28 上午的数据补齐，新的数据库的补齐方法还需要了解。  --ok
3. Grafana 上面对于 Account Net Worth 和 Heartbeat 的 SQL 聚合显示，否则每天传输的数据量太大了，根本干不过来。  
4. 策略层面和执行层面可以分离成两个程序，甚至策略层面可以使用虚拟账户名，由执行层面的程序对各个账户各自承担一部分的敞口。这个可以用 SpiritCatcher 的保存和写入功能来实现。  
5. 执行层面需要有观望模式，比如说我在 grafana 上面按下一个按钮之后，它就只是单纯监控账户变化，但是不再自己下单任何东西。
6. 关于清空仓位的一件按钮的方法，然后清空仓位的过程里面，有两种意义上的清理，比如说带有成交持仓比的清理，还有彻底无视问题的清理。  
7. 11-26 这一天有几个瞬间的 net worth 还是出现了毛刺，我这个还需要再加强一下。基本上是程序刚刚启动，没有足够的市场数据的时候产生的毛刺。  --ok

### 11-27  
今天 1915 有一个重启是因为，在报单之前没有触发熔断，但是在报单的瞬间触发了熔断，结果它在熔断里面重试了好几次也没有成功交易。我觉得在 SpiritCatcher 里面的重试可以换一个方法，就是不行的话，就退回 SpiritCatcher 的层面，而不是一次次在 OptionBase 层面下单。  
这种情况真的叫做，常在河边走。  
这样的话 retry 可以更多记录在数据库里面吗？retry 记录的关键是 ref ，contract 和 limit price 之间的关联。

OrderChange 也需要上传，然后上传的过程里面我们记录下它的第一次出现的时间，远程响应的时间，和消失的时间。  

### 12-01  
Order Update Record 的设计是一个比较麻烦的任务。  
这里的关键是怎么设计一条条更新的状态记录。  
比如说我需要几个时间  
inserted_at, canceled_at, updated_at, first_filled_at, last_filled_at,  

目前还是按照一条一条日志风格的写入，至于聚合整理交给其他的 SQL 脚本逻辑。

```
create table if not exists hb.order_record (
    id serial primary key,
    dt timestamptz not null default now(),
    trade_date date not null, -- 订单记录的日期，方便查询
    username text not null,
    order_ref integer not null,
    order_sys_id int8,
    contract_tradecode text not null,
    contract_name text,
    underlying text,
    direction int2 not null,  -- 1 买入，-1 卖出
    open_close int2 not null, -- 1 开仓，-1 平仓
    -- OPEN, CLOSE, CLOSE_TODAY, CLOSE_YESTERDAY
    offset_flag text not null,  
    order_amount integer not null,
    -- LIMIT, MARKET, IOC, FOK
    order_type text not null,  
    limit_price float8,
    -- RISK_BLOCK, ACCEPTED, REJECTED, PARTIAL_FILLED, FILLED, CANCELED
    order_status text not null,  
    error_code int,
    is_canceled boolean not null default false,  
);
```

### 12-02  
现在的 TODO 又有变化了，现在变成了需要解决它那个期货交易品种特别多，需要经常一直换月的问题。  
所以从订阅到生成期权链都需要自动化地解决问题。  
而且我还需要一个每日验算仓位是否正确的脚本，我还是担心会有逻辑上面的遗漏。  

我感觉那个应该用反向订阅的方法来完成，就是读到了什么特殊的品种再去订阅那个品种。这个可以通过多提供一些接口的方法来完成。  
这个已经完成了。

### 12-11  
信号回测需要 11-25 11-28 的 minute candle 信息，这两天的数据库聚合相关的东西我还没有完成。  
今天赶紧加急一下。  

### 12-23  
今天需要解决 Hummingbird 在初始化时候的错误 net_worth 上报的问题。  --ok
然后需要解决错误开仓之后一件清零的那种 fallback 处理机制和预案。在 11-26 的列表里面说到需要有几种不同的手动清空仓位的方法。比如说关注成交比的锁仓清空。  
然后需要在 clang-tidy 里面添加未初始化的成员对象的警告项目，发现了一个 FutureState 没有初始化指向 Future 的函数的问题。  --ok
然后需要做 2024 年的收益回测。  --import ok  
这个收益回测有很多比想象中更加复杂的事情。

明天要换月了，早上一定要看清楚它应当开仓的是下一个月的合约，是 1 月份的合约。  

### 12-24  
trade date 在 record 系统里面的上传功能还没有做出来，现在的 trade date 是根据交易时间推断的，没办法跳过节假日的时间。  

关于 roll args 挑选的问题，现在是参数特别多，但是我应该可以绘制一个简单的直方图，收集不同收益率的参数组合的数量。  
现在的参数放在了 clip_trade_args 表格里面，这个表格里面有 clip_method, date_interval, 还有 long open close / short open close 参数。  

### 12-25  
关于滑点分析，比较重要的方法是，先得到合成期货的 ETF 价格，我们用一个特殊的品种来储存这个价格。  
然后需要分析之前的 spot code ，比如说用 159915_202512 表示这一个月的期货价格。我们统一一下名称表示的方法。  
更新的期货价格之后，有两个地方可以更新，一个是回测的过程，这个更换 sig_worth 和 signal_combine 脚本里面读取 spot price 的代码，另一个是 cpr 选参数的过程，这个换不换区别都不大。  
我们 cpr 选参数过程，如果要切换成期货价格的话，那么需要重启计算 trade profit 和 roll 的过程。
更换到期货价格主要是为了滑点分析。

### 12-27  
现在从大方向上的事情有这样一些：  

1. ETF 期货价格的演算和回测系统的改进，在回测的过程里可以制造一种图表分析盈亏的构成要素。  
2. 每日启动程序的服务化。这个分成两个阶段，  
    a. 一个是自动启动意义上的服务化，现在每天有九个程序，都在命令行里面启动迟早发疯。  
    b. 一个是 CI/CD 对于稳定版本和 Fallback 版本进行封装交付运行的自动化系统。  
3. 分离 Market Data 的输入过程，因为账户越来越多之后每个账户各自订阅市场数据太占带宽了。  
4. 关于 CPR 的 Rolling 演化问题，  
    a. 我们可以用染色排序，对比上周的排序和这周排序的变化过程。  
    b. 删除一些可用的参数之后做回测，看看和之前的稳定度，从而判断某些参数的结果是否类似。  
    c. 寻找历史排名稳定的参数，或者说寻找一种区间，这个排名区间的参数不会在下一周里面变化太大。

一些小的分支任务有：  

1. Stock 和 Combined 信号的历史回测。  
2. 关于期权回测系统能不能有更加自动化的封装，对于 Nautilus 提供的功能更加自动化选择输入输出。  
3. record trade date 字段上传。 -- ok
4. 交易过程里面需要用挂单代替吃单，  -- ok
这个改造过程的核心是下单失败之后的反馈，  
这个反馈需要从底层提交到 SpiritCatcher 层面，让她持续下单。我想想。  
AbortOnCancel 设置成 false，然后赶紧追加 OrderChange 的变化日志记录，  
然后下单层面的 SendOrders 组合应该用不一样的 Journal 系统。

5. 关于持仓比的保证金具体需要留多少，如果保证金一旦用完，程序应当如何回应。  
如果资金不足以完成底层的开仓请求，那么程序应该在哪个层面回应？  
比如说这个重试返回到了 SpiritCatcher 层面，它报告原因是资金不足。  
那么 SC 是不是应该停止调仓？  
SC 模块是不是应该上报到某种状态日志数据库里面？  

这个过程涉及到 TradeJournal 和集群下单模块的改造。  
集群下单模块的改造需要分离 OptionBase 和 SendOrders 模块。

### 2025-12-29  
今天解决了 OrderChange 上传的问题，整理了整个 order status 变化的过程，整理了 Huaxin 和 CTP 里面 Order status 赋值的错误。然后解决了很多小问题。包括 trade date 不一致。包括 commercial 的交易时间判断，包括 offset close 的自动检查等等的问题。  
这个比我想象中累太多了。  
CTP 一个成功交易的单据有五次更新，第一次是 local echo，第二次是 send echo，第三次是柜台反馈，第四次是交易所反馈，第五次是成交反馈。  
如果是 local risk block 那么会有两次反馈，local echo 和 block echo。  
如果是 OptionBase SendOrders 路径发送的单据，它在第一次下单的时候不会经过 local echo ，但是在 cancel 重新下单的时候会经过 local echo 变化。所以 OptionBase 提交的 local echo PENDING_LOCAL 状态可以同来表示重试的次数。   
但是这不是一个好的设计，我还是应该用其他的一些标志物表示重试次数，用 CANCELED 会更好，只要没有额外的手动交易带来的取消。
现在的 order_record 表格里面，如果是程序自己发出的单据那么至少有两个 PENDING_NEW 状态的回复，如果是其他客户端发送的单据，那么只会有一个 PENDING_NEW 是柜台的反馈。

今天解决了 market data tick aggr 中间的 last price null 的问题。今天还解决了 market price grafana 图表的显示问题。今天还补上了 0259 账户的 trade hold ratio 空缺。

### 2025-12-31
有一个很重要的弱点，目前的弱点是改仓位中途重启，这个重新计算仓位的功能，现在几乎没有方案实现，这个绝对会在将来的一天出事。
这个想一想怎么做，我们需要在计算仓位的时候，保存上一轮计算的输入 max ，如果输入的 max 有变化的话，那么就应该重新计算仓位。但是没必要重新 pick 期权，这个还挺复杂。

