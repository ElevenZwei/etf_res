# TODO  

## 实盘数据录制
可以做的事情有很多  
把上一个录制机器搬到这边，然后做成 user 服务。  
注意数据的写入量问题，重复的数据还是不要写入比较好。  
昨天 2025-07-24 完全没有录制上，因为这个软件的设置需要日常更新。  
所以需要 Wind 弥补一下这一天的 8 9 12 月数据。  

新的录制数据库要怎么设计还是一个问题。  


## Call Put Ratio  
这个模型还有几个方向可以分析，  
对于郑心棠现存的交易方案，有这样几件事，  
一个是现有的几千组参数的调优处理，高维空间里面的聚点特征分析，  
一个是通过几千组参数的资产组合，获得低波动的高回报。  
这里面有一点就是 sum profit 单利和复利的相关程度分析。  

对于我原先的微观曲面模型，有这样几件事  
一个是分析时间切片归一化之后的分布稳定性  
一个是把之前的日内放大波动的曲面转换成归一化曲面  
然后使用已有的工具度量分布特征。  

这些事情其中很重要的一环是数据库的设计，  
良好设计的数据库可以轻松支持上万组参数的分析。  

## Daily Task

### 07-31 Task  
第一个问题是我们现在要做先后优劣程度的分析。  
分离做多和做空，简化到 open close -> profit 的三维曲面，  
对于 long short 还有不同的月份都要分别画一画。  
每月排名前五十名的策略统计一下其他月份的平均表现。  
我们想一想怎么从数据库里面取出对应的数据。  
先安装一个 neovim sql 插件。  

sql rolling args

### 08-05 Task  
现在有两个方向，一个是测试它混合的交易信号在期权上面的表现。  
另一个是分析它的收益曲面，这个需要提取目前参数之间的相关性，  
尝试聚合分离这些参数，从高位表达转向低位空间。  
在高维到低维转向的过程里面我们可以使用哪些工具呢？  

混合交易信号首先本身并不容易合成，我想一想它需要的记录方案。  
Rolling Args 表格本身的设计没有什么太大难度，  
Roll Result 要怎么合成出小数仓位又是一个复杂的运算工作。  
合成的最简单方法是直接带着 Frozen zone 去叠加 cpr.clip_trade_backtest 里面的 position 。  
这样就是数据读写量会大一点。

### 08-06 Task  
滚动组合回测的代码需要有几个地方。  
一个是 roll method 的代码  
一个是 roll args 的输入设计，另外还有运行时间范围的参数  
一个是 roll result 的结果表示和储存。  
一个是读取 clip_trade_profit 之后需要比较整合的过程，  
我们可以提供一个函数，输入所有需要提取的编号和时间段，  
返回这些编号这段时间的交易记录。  

运行 Roll 的时候，根据 trade_args_from_id / to_id 的数值，  
先读取这一段的 trade_args 表格内容。  
把这些表格内容输入 Roll Method 进行第一轮的筛选，  
这一轮筛选完成之后的列表，加上输入的时间窗口，  
获取到这段区域的 clip_trade_profit 或者 clip_trade_backtest 表格，  
把这段数据再输入 Roll Method 代码，  
让它按自己的评估逻辑做出排序结果，或者输出其他的分析报告。  
所以这个模块不仅可以滚动，也可以做其他的分析作用。  

这个排序会定期重置，所以它需要分段调用排序。  
我们可以在外层先调用分片函数，Roll Method 需要提供一个分片逻辑。  
分片逻辑先把时间分成片，然后在每一个时间片上调用排序函数。  
这是静态分片的方法。  

也许需要有根据现有的排序结果里面的收益变化，改变分片长度的动态触发。  
为了满足动态的分片和动态的排序，不是每个分片单独孤立的静态算法排序。  
我们在一轮的排序触发的时候，应当告知上一轮的排序结果在验证集上的表现  
然后它给出新的排序，和这些排序维持的时间，时间到期的话再触发一轮排序。  
或者说它给出的排序一旦验证集上触发了什么条件，那么立刻进行新的一轮排序，不等到期。  
所以这就是另外一种不同的 Roll 运行形式了。  

这两种形式再 cpr.roll_method 表格里面使用 is_static 数据列区分。  
is_static = true 的运行需要 args 里面提供 validate_days, train_days_factor。  
它会使用你提供的时间长度进行分片，  
train_days_factor 表示训练集相对于验证集的时间长度的倍数。  
当 validate_days 是 7 的时候，自动把开始的日期对其每周。  
当 validate_days 是 30 的时候，自动把分片对齐到每月。  
它分片完成之后，在每一个分片上调用 roll method sort 函数得到这一段验证集的预测排序。  
排序的输出需要权重，所以它应当输出一个 DataFrame, 这个 DataFrame 里面需要三列。  
再加上一个分数评估用于日后的比对。  
id(int), rank(int), weight(int), score(float)  

我们先把上面说到的部分完成。

### 08-07 ~ 08-11 Task  
现在主要的问题是需要为了实盘做准备，实盘十组参数会有十个开平仓的时间。  
于是我需要一个叠加参数的，从实盘数据库抽出数据，读取叠加的参数，然后开出单子。  
目前设计的工具流需要这样几个方向，先从总体的流程上思考。  

实盘信号程序和实盘交易程序分开，实盘信号程序负责得出和储存交易信号，转换成现在的仓位数值，实盘交易程序负责把这个仓位数值转换成仓位。  
实盘信号程序需要读取十组参数，这十组参数需要上个星期的回测得到。  
回测的数据需要从录制数据库导出分钟级别的 Call Put Open Interest 还有 ETF 价格。  
目前这段导出的代码是 `./datavis/oi_scripts/db_oi_csv.py` 。  
然后还需要导入 cpr 数据库，这一步需要自动化起来。  
然后从 cpr 数据库运行回测也需要自动化。  
之后是导出 cpr 回测得出的数组，这是一个参数编号数组。  
这个数组需要转换成 cpr ratio diff 的触发数组。  
关于触发数组如何转换成仓位的问题，我需要写一个 python 代码，  
这个 python 代码要在回测数据上做验证，保证结果和我们的多组平均是一致的。  
然后这个 python 代码要保存到实盘数据库，交易程序要读取这个实盘数据库。  
交易程序从实盘数据库到仓位的转换变化过程可以参考 Nautilus 回测代码。  

另外现在得到十组参数的代码，因为要写入 roll_rank 表格， 所以它需要已经有完整的验证集数据。  
但是这样就不能预测实盘参数了，所以我需要设置一个模式不写入 roll_rank 的单纯预测模式。  
然后是得到十组参数的代码现在有很强烈的相似性，分离这些不同参数的收益特征需要 PCA 统计。  
经过大量的去重之后得到有差异性的参数组合才能反映问题。  
这个过程如何进展？这一点还需要很多的统计。

上面说的是流程上的东西，然后我们需要架构上的设计。  
实盘机器用哪个？录制市场数据的数据库用哪个？信号计算放在哪个地方执行？交易信号的数据库用哪个？  
这里问题的关键是我们使用 CTP Option 互联网实盘还是使用 Huaxin 实盘。


这个真的是任重而道远。

我需要整理一下各种鬼地方的 Wind 下载代码，  
现在有好几个地方，我应该汇总一下功能，  
或者应该先研究工具流的问题？比如说 RIME 输入法的配置同步。  
仔细判断一下现在数据流的输入问题，我们使用的数据如果和 Wind 下载的数据做一个对比？  

### 08-12 Task  
昨天搞定了 Position Merge 的代码，今天进一步思考实盘运行起来的必要条件。  
实盘运行的方式现在暂定成 Export 所有需要的参数，然后新建一个脚本运行在 Record Server 上面。  

Export 是一种整理信息的方式，即使不做 Export 在程序运行的时候差不多也要经历一个 Export 过程。  
Export 需要的参数这个会真的很多很长。  
比如说有十组参数，每一组都需要每一分钟的分布，  
或者说每一分钟的触发阈值，所以说这需要一个 Dict 结构。  
像是 cpr.clip_trade_backtest 表格里面的一样。  
我们用这样的结构表示：  

```json
{
    "dataset_id": 1,  
    "input_dt_from": "2025-01-13 00:00:00",
    "input_dt_to": "2025-01-14 23:59:59",
    "interval": 60,  
    "roll_args_id": 1,  
    "roll_dt_from": "2025-01-13 00:00:00",
    "roll_dt_to": "2025-01-19 00:00:00",
    "roll_method_id": 1,
    "roll_method_name": "best_return",  
    "roll_method_variation": "logret_t1w_v1w",  
    "roll_method_json": "<json dump>",  
    "roll_trade_args_from_id": 1,  
    "roll_trade_args_to_id": 8082,
    "spotcode": "159915",
    "top": 10,  
    "trade_args": { "1000": 0.1, "1001": 0.1, ... }  
    "trade_args_details": [  
        {  
            "trade_args_id": 1000,  
            "trade_args_json": "<json dump>",  
            "trade_args_thresholds": {  
                "long_open_threshold": 0.3,  
                ...  
            },  
            "trigger": [  
                {  
                    "time": "09:35:00",  
                    "long_open": -0.002,  
                    "long_close": 0,  
                    "short_open": 0.002,  
                    "short_close": 0,  
                },  
                { ... },
            ],  
        },  
        { ... },
    ],
}
```

交易程序读取里面的 trade_trigger 数组，然后把它转换成 DataFrame，存放在指定的 spotcode 名下。  
当触发 spotcode 的新行情的时候，检查这些 DataFrame，如果这个 DataFrame 里面有分钟数字对齐的条目，那么接着检查这个 DataFrame 的 open close 数字，和当前行情的 `(c-p)/(c+p)` 的日内变化量做比较。如果触发的话，表示这个 trade_args_id 需要变换仓位。  
每个 trade_args_id 的仓位从 0 到 1 到 -1 ，在输出的时候再按照权重加权计算总和。  

触发 spotcode 新行情的方案，按照 interval 字段的间隔触发，不过为了和回测对齐，在触发的时候应当尽可能贴近分钟开始的时候，考虑到延迟等等情况，我们可以在 5 秒的时候检查数据库里面是不是有新的行情。或者说经常查看行情数据库，在看到新的行情和上一次触发时间之间的时间差满足条件的时候就触发。  

每次检查触发的最后，给当前的状态 DataFrame 新增一行每个 trade_args_id 的检查结果，并且把这个 DataFrame 保存到 csv 文件里面。csv 文件的命名不能重复，例如用启动时间加上 spotcode 命名。  

```text
CSV 数据格式表达。
filename: cpr_pos_spot_159915_dt_20250808_092000.csv
csv header: dt, spotcode, position, ta_1234, ta_1235, ...
```

这个记录储存在数据库里面的话就需要使用一个转轴的写法了：  

``` text
SQL 数据格式表达。
dt, spotcode, trade_args_id, position, weight  
其中 trade_args_id is null 并且 weight = 1 的数据行表示仓位总和。
```

检查触发之后还需要其他的形式通知交易软件，要是交易软件每次都去读取文件 File Descriptor 的变化那就有一点太复杂了。那不如我们单独搞一个数据库，例如 Redis 共享 Dict 的方式，或者 SQL 查询最新一行的方式。  

于是今天我们把配置导出和读取市场数据的这样两步给解决了。  

Export 脚本需要读取 Roll Result 表格的数据，然后查询每个 trade_args_id 的详细配置，再从 cpr.clip 里面读取对应的分布情况。好像相当复杂。  
这个脚本有很大一部分功能和 `cpr_diff_sig.py` 是重叠的。  
Export 脚本为了得到触发表格，它的日期范围只能限制在同一个星期里面，用日期加上 trade_args 里面的 date_interval 数字得到分布采样的日期范围。然后再加每一分钟得到 dt_range_id。然后再查询 `cpr.clip` 表格得到采样数据。用这个采样数据和 `cpr.clip_trade_args` 里面的触发条件做比较，得到具体的触发阈值。  

现在 `clip_trade_args` 里面的 args json 是这个形式。
```json
{
  "method": "min_max",
  "variation": "default",
  "arg_variation": "c45_lo30_lc10_so20_sc0",
  "date_interval": 90,
  "zero_threshold": 0.45,
  "long_open_threshold": -0.3,
  "long_close_threshold": -0.1,
  "short_open_threshold": 0.2,
  "short_close_threshold": 0
}
```

### 08-13 Task  

Export 得到的阈值首先需要和 clip_trade_backtest 里面的数列进行对比，确定得到的触发边界是不是一致的。嗯，验证过了，是相同的。  

Export 的脚本终于搞定了之后，下一步是需要解决这个脚本的运行机制，它需要可以从数据库里面读取指定时间的 Open Interest 数据，自行计算仓位变化的信号，然后我需要把这个仓位变化的信号和我的 roll_merged 表格里面的数据对比。

`src/json_run.py` 做出来的结果看起来和 roll_merged 表格里面是一样的，这样很安心。下一个需要验证的是 roll_merged 仓位的收益是否和 Logret 报告的复利收益一样。  

### 09-12 Task  
实盘有几个 Critical 级别的问题，一个是关于 Market near open close 的判断问题。后来发现是 Huaxin Ctp Options 接口特有的得不到 ETF 合约详细信息导致的检测漏洞，已经修复。  
一个是总体仓位保证金的监控和报告机制。  
一个是各个策略的营收表现的报告机制。  
这些都做成一个个 Spirit 模组。  
另外一个老大问题是录制机器的移动，这个优先级目前往上调。  

### 09-15  
关于安全警告性质的监控系统。  
有几个方面可以做，一个是说关于之后一个交易日是否有足够参数的检查。一个是关于持续运行的时间和数据库里面是否有合理数据的检查。  
然后比如说周五下午可以自动执行一次 weekly update ，这个脚本自己也需要内部检测，保证每天都有充分数量的信号，不是哪一天特别残缺，所以者需要先经过一个数据完整性的检测。  

### 09-18  
今天折腾了一天才知道在本地运行华鑫内网交易程序的办法。  
基本上是使用 `ssh -D <port>` 建立 socks5 隧道，然后使用 proxychains 代理所有的程序出站连接，但是 127.0.0.1 的除外，从而保证对本地信号数据库的连接。  
仅仅转发一两个端口的数据是没有作用的，它会使用的端口不限于那一两个，仅仅一两个端口就会报告连接断开，发送失败等等。当我把 hx 机器上的程序复制下来发现没有办法运行的时候，我就意识到了这个问题。  
同时 hx_yl 这个机器上不能运行产品号以外的任何账号，所以我只能从 hx_yl 机器上面监听行情，但是不可以发出交易。  

目前的情况是我建立了整个 proxychains 配置文件之后使用脚本启动交易程序，这样就可以正确运行了。  
我最好需要判断一下这个程序建立了哪些连接。这个可以用 proxychains 来帮助我记录吗？proxychains 有没有这样的日志功能，像是 proxifier 一样？  


另外一条路线是在服务器里面配置完整的运行系统。
在服务器上面配置完整的交易系统遇到了这样几个方面的阻碍：  

1. 编译的时候 CPM 需要使用 env proxy 才能连接互联网。使用 `ssh -R 7888:localhost:7888` + export proxy 的方法。  
2. pqxx 需要依赖 libpq 才能使用，在没有系统包安装权限的场景里面需要手动编译 libpq 。  
3. libpq 并没有单独精简的 git 仓库，它是和 postgresql 混在一起编译的。它会依赖一部分 postgresql 的编译内容。而 postgresql 编译的依赖项目比较多。有 icu, lex, bison 这些开发工具。这些东西如果不能装系统包的话都需要一个个手动编译。  
4. 这些手动编译的工作量一点也不小。而且编译之后都放在 `~/.local/` 文件夹里面，然后需要配置系统环境变量让其他的工具可以搜索到这个位置。  


我不知道上面这些配置过程可否使用 AI Agent 工具自动搜索资料下载编译试错解决问题等等。  

### 09-19  
为了将来自动化的表达，我还需要解决这样一些问题：  

1. viu ssh 图片显示，搭配上 df plot 和 ai copilot 一起使用，不再重复本地化的 Excel 绘图工作。  
2. 自动化回测结果的报表展示，使用上生成回测结果分析的软件包。  
3. 每天要启动的软件的日常服务化。  
4. Hummingbird 启动之后写一个 lockfile，然后 grafana 里面也要监控好打点输出的情况防止多开。  


